<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machinelearn on TOM&#39;s zone</title>
    <link>https://hasaki.xyz/tags/machinelearn/</link>
    <description>Recent content in Machinelearn on TOM&#39;s zone</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ZH-CN</language>
    <copyright>Early 2016 ~ 2020 &amp;copy; TOM&#39;s Zone</copyright>
    <lastBuildDate>Thu, 09 Jan 2020 11:13:40 +0800</lastBuildDate>
    
	<atom:link href="https://hasaki.xyz/tags/machinelearn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>浅读《机器学习实战》</title>
      <link>https://hasaki.xyz/blog/2020-01-09-%E6%B5%85%E8%AF%BB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</link>
      <pubDate>Thu, 09 Jan 2020 11:13:40 +0800</pubDate>
      
      <guid>https://hasaki.xyz/blog/2020-01-09-%E6%B5%85%E8%AF%BB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/</guid>
      <description>机器按照是否 分类 和 回归 可以分为两种：
 监督学习  监督学习需要分类和回归，这类算法必须知道预测什么，即目标变量的分类信息。
监督学习的用途有：
 k-近邻算法（KNN） 朴素贝叶斯算法 支持向量机 决策树 线性回归 局部加权线性回归 Ridge 回归 Lasso 最小回归系数估计  我们在本文当中会逐个说到上诉用途
 非监督学习  与监督学习相对应的是无监督学习，此时数据没有类别信息，也不会给定目标值。在无监督 学习中，将数据集合分成由类似的对象组成的多个类的过程被称为聚类；将寻找描述数据统计值 的过程称之为密度估计。
此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。
无监督学习的用途有：
 K-均值 DBSCAN 最大期望算法 Parzen 窗设计  KNN  kNN（k-Nearest Neighbor）算法的核心思想是如果一个样本在特征空间中的 k 个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别
 KNN 是机器学习中最简单易懂的算法，它的适用面很广，并且在样本量足够大的情况下准确度很高，多年来得到了很多的关注和研究，K-近邻算法采用测量不同特征值之间的距离方法进行分类。适用数据范围：数值型和标称型
kNN 当中的 k 取不同值时，分类结果可能会有显著不同。
 优点：简单，易于理解，易于实现，精度高、对异常值不敏感、无数据输入假定。 缺点：计算复杂度高、空间复杂度高。  下面是 k-近邻算法当中带有 4 个数据点的简单例子，例子当中对 4 个数据点进行了简单分类为 A 和 B：
import numpy as np import matplotlib.</description>
    </item>
    
    <item>
      <title>Tensor &amp;&amp; flow</title>
      <link>https://hasaki.xyz/blog/2019-08-09-tensor--flow/</link>
      <pubDate>Fri, 09 Aug 2019 13:22:30 +0800</pubDate>
      
      <guid>https://hasaki.xyz/blog/2019-08-09-tensor--flow/</guid>
      <description>本文只是简单 TensorFlow 的一些理解和介绍，并不包含详细的安装、API、以及更多深层内容。
 首先简单说一下 标量（Scalar）、向量（Vector）、张量（Tensor）。
下面会以 physical student 和 cs student 两个角度来解释。
Scalar 在物理学角度，标量是遵循简单代数规则的一些量，与坐标系的选取无关。例如，加法，减法等。标量也可以被定义为仅需要识别幅度的量。例如温度，电流等
CS 里面可以简单认为是一个常量（constant）
Vector 物理学角度向量是空间中的箭头，也可以是矢量。
决定一个向量的是它的长度（length）和它所指的方向（direction），而且只要以上特征相同，你一个任意移动一个向量而保持它不变。比如速度和力。而且平面中（平面直角坐标系）的向量是二维的，处于我们生活当中的向量是三维的。
举个例子，一个二维向量 [-2,3] 以平面直角坐标系来表示为：
 第一个数代表沿 X 轴走多远，第二个数代表沿 Y 轴走多远。
 图中的黄色箭头就是向量，可以看到一个向量都有一个唯一一对对数
而一个三维向量 [2,1,3] 以平面直角坐标系来表示为：
 第一个数代表沿 X 轴走多远，第二个数代表沿 Y 轴走多远，第三个数代表沿 Z 轴走了多远
 CS 角度向量是一个二维数组（二维）、三维数组（三维）或者说是矩阵。代表有序的数字集合。
线性代数 向量在线性代数当中可以叫做矩阵（matrix），矩阵可以做一些基本的运算，对应的加法和乘法规则是：
 两个矩阵行列数必须要相同才能进行求和运算。   两个矩阵要相乘，前一个矩阵的列数必须要等于后一个矩阵的行数，也可以看成是列的加权求和   至于更多的矩阵求逆、求导超出本文的范畴，请自行学习线性代数。
 我们也可以换一种方式来看向量的坐标，把一个 [1,1] 的向量定义为 基向量。
上文的二维向量 [-2,3]，对应的每个坐标可以看作一个为 Scalar（标量）。这个向量可以看作把基向量按照标量的大小进行缩放所产生。-2 代表往右 2 倍，3 代表向上 3 倍</description>
    </item>
    
  </channel>
</rss>